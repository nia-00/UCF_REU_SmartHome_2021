{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import csv\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow.keras as keras\n",
    "from keras import layers\n",
    "from sklearn import preprocessing\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "print('done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put everything into a dict\n",
    "\n",
    "# make list of file names\n",
    "file_names = ['charlotte_sim_1_cooltoofast_combined_interpolated.csv', 'charlotte_sim_1_lowpeaktemp_combined_interpolated.csv',\n",
    "              'charlotte_sim_1_lowtemps_combined_interpolated.csv','charlotte_sim_1_successful_combined_interpolated.csv', \n",
    "              'charlotte_sim_2_successful_combined_interpolated.csv',\n",
    "              'charlotte_sim_2_v2_combined_interpolated.csv', 'charlotte_sim_3_combined_interpolated.csv',\n",
    "              'denver_sim_1_combined_interpolated.csv', 'denver_sim_2_combined_interpolated.csv','denver_sim_3_combined_interpolated.csv',\n",
    "              'detroit_sim_1_combined_interpolated.csv', 'detroit_sim_2_incomplete_combined_interpolated.csv', \n",
    "              'detroit_sim_2_successful_combined_interpolated.csv','detroit_sim_3_combined_interpolated.csv',\n",
    "              'jacksonville_sim_1_combined_interpolated.csv', 'jacksonville_sim_2_combined_interpolated.csv',\n",
    "              'jacksonville_sim_3_combined_interpolated.csv', 'lasvegas_sim_1_combined_interpolated.csv',\n",
    "              'lasvegas_sim_2_combined_interpolated.csv', 'lasvegas_sim_3_combined_interpolated.csv']\n",
    "\n",
    "# declare the dict \n",
    "file_dict = dict()\n",
    "file_dict_len = len(file_names)\n",
    "# loop through file names, open then, convert to list, and add to dict as a numpy array with i as the key\n",
    "for i in range(file_dict_len):\n",
    "    with open(file_names[i], \"r\") as f:\n",
    "        reader = pd.read_csv(f)\n",
    "        # exclude heater due to errors and sensor/motor times\n",
    "        reader.drop(['heater','Sensor Time', 'Motor Time'], inplace = True, axis = 1)\n",
    "        file_data = [list(x) for x in reader.values]\n",
    "        file_dict[i] = file_data\n",
    "# check the conversion\n",
    "# print(file_dict[0])\n",
    "print(\"done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine train files and test files into separate lists\n",
    "full_train_x_set = [None] * 4500\n",
    "full_train_y_set = [None] * 4500\n",
    "full_test_x_set = [None] * 2000\n",
    "full_test_y_set = [None] * 2000\n",
    "trainInd = 0\n",
    "testInd = 0\n",
    "\n",
    "for i in range(0,file_dict_len - 5):\n",
    "    for j in range(0,len(file_dict[i])):\n",
    "        #print(file_dict[i][j])\n",
    "        full_train_x_set[trainInd] = file_dict[i][j]\n",
    "        full_train_y_set[trainInd] = file_dict[i][j][0:16]\n",
    "        trainInd += 1\n",
    "\n",
    "for i in range(file_dict_len - 5, file_dict_len):\n",
    "    for j in range(0, len(file_dict[i])):\n",
    "        full_test_x_set[testInd] = file_dict[i][j]\n",
    "        full_test_y_set[testInd] = file_dict[i][j][0:16]\n",
    "        testInd += 1\n",
    "\n",
    "# filter the Nones out of the lists\n",
    "filtered_train_x_set = list(filter(None,full_train_x_set))\n",
    "filtered_train_y_set = list(filter(None, full_train_y_set))\n",
    "filtered_test_x_set = list(filter(None,full_test_x_set))\n",
    "filtered_test_y_set = list(filter(None,full_test_y_set))\n",
    "#print(filtered_train_set[len(filtered_train_set) - 5: len(filtered_train_set)])\n",
    "\n",
    "# scale all the data \n",
    "x_scaler = MinMaxScaler()\n",
    "y_scaler = MinMaxScaler()\n",
    "scaled_train_x = x_scaler.fit_transform(filtered_train_x_set)\n",
    "scaled_train_y = y_scaler.fit_transform(filtered_train_y_set)\n",
    "scaled_test_x = x_scaler.transform(filtered_test_x_set)\n",
    "scaled_test_y = y_scaler.transform(filtered_test_y_set)\n",
    "\n",
    "#print(scaled_train[len(scaled_train) - 5: len(scaled_train)])\n",
    "#print(scaled_test_y)\n",
    "print(\"done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seed so this can be reproduced\n",
    "seed_val = 7\n",
    "def reset_random_seeds():\n",
    "    tf.random.set_seed(seed_val)\n",
    "    np.random.seed(seed_val)\n",
    "    random.seed(seed_val)\n",
    "    \n",
    "reset_random_seeds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting some specifications for model based on our data\n",
    "num_features = 34 # i think this is number of variables but im not 100% sure\n",
    "batch_size = 64 # the number of chunks of data being fed into the ml algorithm at a time (always a power of 2)\n",
    "time_steps = 10 # the size of each chunk of data being fed into the ml algorithm\n",
    "shift_steps = 15 # the number of cells to shift the y values so the predictions line up with the current data\n",
    "train_percent = 0.8 # the percentage of the input files to set aside for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ** converting all training data into batches and putting it into a file **\n",
    "# use the lengths of the files to get the indicies\n",
    "print('start')\n",
    "def train_batches(time_steps, shift_steps):\n",
    "    # for each file\n",
    "    train_set = [[None for x in range(2)] for i in range(4096)]\n",
    "    file_sum = 0\n",
    "    ind = 0\n",
    "    for i in range(file_dict_len - 5):\n",
    "        # get num rows \n",
    "        current_len = len(file_dict[i])\n",
    "        # start at i and i + 15 (j)\n",
    "        for j in range(file_sum, file_sum + current_len - shift_steps + 1):\n",
    "            #print(ind)\n",
    "            train_set[ind][0] = scaled_train_x[j:j + time_steps]\n",
    "            temp = scaled_train_x[j + shift_steps - 1: j + shift_steps]\n",
    "            train_set[ind][1] = temp[0][0:16]\n",
    "            #print(train_set[ind][0])\n",
    "            ind += 1\n",
    "        file_sum += current_len        \n",
    "    return train_set\n",
    "\n",
    "train_set = train_batches(time_steps, shift_steps)\n",
    "train_set = train_set[0:len(train_set) - 85][0:len(train_set)- 85]\n",
    "#print(train_set[len(train_set) - 5: len(train_set)])\n",
    "#train_set = np.array(train_set)\n",
    "\n",
    "#with open('train_batches.csv', \"w\", newline = \"\") as x:\n",
    "#    writeX = csv.writer(x)\n",
    "#    writeX.writerows(train_set) \n",
    "    \n",
    "print('done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ** converting all testing data into batches and putting it into a file **\n",
    "print('start')\n",
    "\n",
    "def test_batches(time_steps, shift_steps):\n",
    "    # for each file\n",
    "    test_set = [[None for x in range(2)] for i in range(2046)]\n",
    "    file_sum = 0\n",
    "    ind = 0\n",
    "    for i in range(file_dict_len - 5, file_dict_len):\n",
    "        # get num rows \n",
    "        current_len = len(file_dict[i])\n",
    "        # start at i and i + 15 (j)\n",
    "        for j in range(file_sum, file_sum + current_len - shift_steps + 1):\n",
    "            #print(ind)\n",
    "            test_set[ind][0] = scaled_test_x[j:j + time_steps]\n",
    "            temp = scaled_test_x[j + shift_steps - 1: j + shift_steps]\n",
    "            test_set[ind][1] = temp[0][0:16]\n",
    "            #print(test_set[ind][0])\n",
    "            ind += 1\n",
    "        file_sum += current_len        \n",
    "    return test_set\n",
    "\n",
    "test_set = test_batches(time_steps, shift_steps)\n",
    "test_set = test_set[0:len(test_set) - 545][0:len(test_set) - 545]\n",
    "#print(test_set[len(test_set) - 5:len(test_set)])\n",
    "#test_set = np.array(test_set)\n",
    "\n",
    "#with open('test_batches.csv', \"w\", newline = \"\") as x:\n",
    "#    writeX = csv.writer(x)\n",
    "#    writeX.writerows(test_set)\n",
    "    \n",
    "print('done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle test and train\n",
    "train_set = np.array(train_set)\n",
    "test_set = np.array(test_set)\n",
    "\n",
    "np.random.shuffle(train_set)\n",
    "np.random.shuffle(test_set)\n",
    "\n",
    "# split into x and y\n",
    "x_train = [train_set[i][0] for i in range(len(train_set))]\n",
    "x_train = np.array(x_train)\n",
    "y_train = [train_set[i][1] for i in range(len(train_set))]\n",
    "y_train = np.array(y_train)\n",
    "#y_train = y_train.reshape(y_train.shape[0], y_train.shape[1], 1)\n",
    "\n",
    "x_test = [test_set[i][0] for i in range(len(test_set))]\n",
    "x_test = np.array(x_test)\n",
    "y_test = [test_set[i][1] for i in range(len(test_set))]\n",
    "y_test = np.array(y_test)\n",
    "#y_test = y_test.reshape(y_test.shape[0], y_test.shape[1],1)\n",
    "\n",
    "# check the shapes\n",
    "# print(x_train.shape)\n",
    "# print(y_train.shape)\n",
    "# print(x_test.shape)\n",
    "# print(y_test.shape)\n",
    "print('done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the network\n",
    "model = Sequential()\n",
    "model.add(LSTM(4, input_shape=(x_train.shape[1], x_train.shape[2])))\n",
    "model.add(Dense(16))\n",
    "opt = keras.optimizers.Adam(learning_rate=0.005)\n",
    "#optimizer = SGD(learning_rate = 0.01, nesterov=True, momentum=0.9)\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer = 'adam')\n",
    "\n",
    "# fit the network\n",
    "history = model.fit(x_train, y_train, epochs=50, batch_size=batch_size, validation_data=(x_test, y_test), verbose = 1, shuffle=False)\n",
    "# model.summary()\n",
    "\n",
    "# plot history\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# make predictions\n",
    "train_pred = model.predict(x_train)\n",
    "test_pred = model.predict(x_test)\n",
    "\n",
    "print('done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale the predictions back\n",
    "train_pred = y_scaler.inverse_transform(train_pred)\n",
    "y_train = y_scaler.inverse_transform(y_train)\n",
    "\n",
    "test_pred = y_scaler.inverse_transform(test_pred)\n",
    "y_test = y_scaler.inverse_transform(y_test)\n",
    "\n",
    "# get RMSE\n",
    "trainScore = math.sqrt(mean_squared_error(y_train[:,0], train_pred[:,0])) \n",
    "testScore = math.sqrt(mean_squared_error(y_test[:,0], test_pred[:,0]))\n",
    "print('Train Score: %.2f RMSE' % (trainScore))\n",
    "print('Test Score: %.2f RMSE' % (testScore))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(train_pred.shape)\n",
    "#with open('predictions.csv', \"w\", newline = \"\") as p:\n",
    "#    writeP = csv.writer(p)\n",
    "#    writeP.writerows(test_pred)\n",
    "    \n",
    "print('done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('start')\n",
    "fig = plt.figure(figsize = (15,10), )\n",
    "\n",
    "ax1 = fig.add_subplot(222, label = '1')\n",
    "ax2 = fig.add_subplot(221, label = '2')\n",
    "\n",
    "\n",
    "# make the first axes \"current\" again\n",
    "#plt.subplot(221)\n",
    "labels = [\"S4_Humidity\",\"S4_Temperature\",\"S6_Humidity\",\"S6_Temperature\",\"S12_Humidity\",\"S12_Temperature\",\"S18_Humidity\",\n",
    "          \"S18_Temperature\",\"S19_Humidity\",\"S19_Temperature\",\"S24_Humidity\",\"S24_Temperature\",\"S25_Humidity\",\"S25_Temperature\",\n",
    "          \"S26_Humidity\",\"S26_Temperature\"]\n",
    "for i in range(0,y_test.shape[1],2):\n",
    "    ax1.scatter(y_test[:,i], test_pred[:,i], label = labels[i])\n",
    "    ax2.scatter(y_test[:,i + 1], test_pred[:, i + 1], label = labels[i + 1])\n",
    "#plt.scatter(y_test, test_pred)\n",
    "ax1.plot([45, 65], [45, 65], color = 'red')\n",
    "ax2.plot([69, 81], [69, 81], color = 'red')\n",
    "ax1.set_title('Humudity')\n",
    "ax2.set_title('Temperature')\n",
    "ax1.set_xlabel('Actual')\n",
    "ax1.set_ylabel('Prediction')\n",
    "ax2.set_xlabel('Actual')\n",
    "ax2.set_ylabel('Prediction')\n",
    "fig.tight_layout(pad = 4.0)\n",
    "ax1.legend()\n",
    "ax2.legend()\n",
    "plt.show()\n",
    "fig.savefig('LSTM_no_time_temp_hum_graphs2.png')\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "def grid_search(model, param_dict, train_data, train_y):\n",
    "    grid = GridSearchCV(model, param_dict, n_jobs=8)\n",
    "    grid.fit(train_data, train_y)\n",
    "    print(grid.best_params_)\n",
    "    \n",
    "    return grid\n",
    "\n",
    "rfg_params = {'n_estimators': list(range(75, 225, 25)),\n",
    "              'max_depth': [2,3,4,5],\n",
    "              'criterion':['mse', 'mae']}\n",
    "              \n",
    "'''\n",
    "\n",
    "rfg_params = {'n_estimators': list(range(75, 125, 25)),\n",
    "              'max_depth': [2,3],\n",
    "              'criterion':['mse']}\n",
    "'''\n",
    "\n",
    "svr_params = {'kernal' : ['linear', 'poly', 'rbf'],\n",
    "              'degree': [2,3], \n",
    "              'epsilon': [0.05, 0.1, 0.15, 0.2]}\n",
    "\n",
    "knn_params = {'n_neighbors': list(range(4,11)), 'weights': ['uniform', 'distance']}\n",
    "\n",
    "xgb_params = {''}\n",
    "\n",
    "\n",
    "model = RandomForestRegressor()\n",
    "train_flatten = np.reshape(x_train, [x_train.shape[0],x_train.shape[1]*x_train.shape[2]])\n",
    "train_flatten.shape\n",
    "best_model = grid_search(model, rfg_params, train_flatten, y_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
